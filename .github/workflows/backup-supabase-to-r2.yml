name: Backup Supabase to Cloudflare R2

on:
  schedule:
    - cron: '0 18 * * *'
  workflow_dispatch:

jobs:
  backup:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Install Supabase CLI
        run: |
          curl -L https://github.com/supabase/cli/releases/latest/download/supabase_linux_amd64.tar.gz -o supabase.tar.gz
          tar -xzf supabase.tar.gz
          sudo mv supabase /usr/local/bin/
          sudo chmod +x /usr/local/bin/supabase
          supabase --version
      - name: Install AWS CLI
        run: |
          curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
          unzip awscliv2.zip
          sudo ./aws/install --update || sudo ./aws/install
      - name: Configure AWS CLI
        env:
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
        run: |
          aws configure set aws_access_key_id $R2_ACCESS_KEY_ID
          aws configure set aws_secret_access_key $R2_SECRET_ACCESS_KEY
          aws configure set region auto
      - name: Create backup directory
        run: mkdir -p backups
      - name: Dump database
        env:
          SUPABASE_PROJECT_REF: ${{ secrets.SUPABASE_PROJECT_REF }}
          SUPABASE_DB_PASSWORD: ${{ secrets.SUPABASE_DB_PASSWORD }}
        run: |
          TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
          BACKUP_FILE="backups/supabase_backup_${TIMESTAMP}.sql"
          DB_URL="postgresql://postgres:${SUPABASE_DB_PASSWORD}@db.${SUPABASE_PROJECT_REF}.supabase.co:5432/postgres"
          if supabase db dump --db-url "${DB_URL}" -f "${BACKUP_FILE}"; then
            echo "BACKUP_FILE=${BACKUP_FILE}" >> $GITHUB_ENV
            ls -lh "${BACKUP_FILE}"
          else
            exit 1
          fi
      - name: Compress backup
        run: |
          gzip "${BACKUP_FILE}"
          echo "COMPRESSED_FILE=${BACKUP_FILE}.gz" >> $GITHUB_ENV
      - name: Upload to R2
        env:
          R2_ACCOUNT_ID: ${{ secrets.R2_ACCOUNT_ID }}
          R2_BUCKET_NAME: ${{ secrets.R2_BUCKET_NAME }}
        run: |
          aws s3 cp "${COMPRESSED_FILE}" "s3://${R2_BUCKET_NAME}/$(basename ${COMPRESSED_FILE})" --endpoint-url="https://${R2_ACCOUNT_ID}.r2.cloudflarestorage.com"
      - name: Cleanup old backups
        env:
          R2_ACCOUNT_ID: ${{ secrets.R2_ACCOUNT_ID }}
          R2_BUCKET_NAME: ${{ secrets.R2_BUCKET_NAME }}
        run: |
          ENDPOINT_URL="https://${R2_ACCOUNT_ID}.r2.cloudflarestorage.com"
          CUTOFF_DATE=$(date -d '30 days ago' +%Y%m%d || date -v-30d +%Y%m%d)
          aws s3 ls "s3://${R2_BUCKET_NAME}/" --endpoint-url="${ENDPOINT_URL}" | while read -r line; do
            BACKUP_NAME=$(echo "$line" | awk '{print $4}')
            if [[ "$BACKUP_NAME" =~ ^supabase_backup_([0-9]{8})_ ]]; then
              BACKUP_DATE="${BASH_REMATCH[1]}"
              if [ "$BACKUP_DATE" -lt "$CUTOFF_DATE" ]; then
                aws s3 rm "s3://${R2_BUCKET_NAME}/${BACKUP_NAME}" --endpoint-url="${ENDPOINT_URL}"
              fi
            fi
          done
