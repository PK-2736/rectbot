name: Backup Supabase to R2 (API Method)

on:
  # æ¯Žæ—¥åˆå‰3æ™‚ï¼ˆJST = UTC+9 ãªã®ã§ UTC 18:00ï¼‰ã«å®Ÿè¡Œ
  schedule:
    - cron: '0 18 * * *'
  
  # æ‰‹å‹•å®Ÿè¡Œã‚‚å¯èƒ½
  workflow_dispatch:

jobs:
  backup:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y curl jq

      - name: Install AWS CLI for R2
        run: |
          curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
          unzip awscliv2.zip
          sudo ./aws/install

      - name: Configure AWS CLI for Cloudflare R2
        env:
          R2_ACCOUNT_ID: ${{ secrets.R2_ACCOUNT_ID }}
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
        run: |
          aws configure set aws_access_key_id $R2_ACCESS_KEY_ID
          aws configure set aws_secret_access_key $R2_SECRET_ACCESS_KEY
          aws configure set region auto

      - name: Create backup directory
        run: mkdir -p backups

      - name: Request Supabase backup via Management API
        env:
          SUPABASE_ACCESS_TOKEN: ${{ secrets.SUPABASE_ACCESS_TOKEN }}
          SUPABASE_PROJECT_REF: ${{ secrets.SUPABASE_PROJECT_REF }}
        run: |
          TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
          
          echo "Requesting backup from Supabase Management API..."
          
          # Supabase Management API ã§ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
          RESPONSE=$(curl -X POST \
            "https://api.supabase.com/v1/projects/${SUPABASE_PROJECT_REF}/database/backups" \
            -H "Authorization: Bearer ${SUPABASE_ACCESS_TOKEN}" \
            -H "Content-Type: application/json" \
            -w "\n%{http_code}" \
            -s)
          
          HTTP_CODE=$(echo "$RESPONSE" | tail -n1)
          BODY=$(echo "$RESPONSE" | head -n-1)
          
          echo "HTTP Status: ${HTTP_CODE}"
          echo "Response: ${BODY}"
          
          if [ "$HTTP_CODE" = "200" ] || [ "$HTTP_CODE" = "201" ]; then
            BACKUP_ID=$(echo "$BODY" | jq -r '.id // empty')
            echo "âœ… Backup requested successfully"
            echo "BACKUP_ID=${BACKUP_ID}" >> $GITHUB_ENV
            echo "TIMESTAMP=${TIMESTAMP}" >> $GITHUB_ENV
          else
            echo "âŒ Failed to request backup"
            echo "This method requires Supabase Management API access"
            echo "Alternative: Use pg_dump with IPv4 NAT or local execution"
            exit 1
          fi

      - name: Wait for backup completion
        if: env.BACKUP_ID != ''
        env:
          SUPABASE_ACCESS_TOKEN: ${{ secrets.SUPABASE_ACCESS_TOKEN }}
          SUPABASE_PROJECT_REF: ${{ secrets.SUPABASE_PROJECT_REF }}
        run: |
          echo "Waiting for backup to complete..."
          
          MAX_WAIT=600  # 10åˆ†
          ELAPSED=0
          
          while [ $ELAPSED -lt $MAX_WAIT ]; do
            STATUS_RESPONSE=$(curl -X GET \
              "https://api.supabase.com/v1/projects/${SUPABASE_PROJECT_REF}/database/backups/${BACKUP_ID}" \
              -H "Authorization: Bearer ${SUPABASE_ACCESS_TOKEN}" \
              -s)
            
            STATUS=$(echo "$STATUS_RESPONSE" | jq -r '.status // "unknown"')
            
            echo "Backup status: ${STATUS}"
            
            if [ "$STATUS" = "completed" ]; then
              DOWNLOAD_URL=$(echo "$STATUS_RESPONSE" | jq -r '.download_url')
              echo "âœ… Backup completed"
              echo "DOWNLOAD_URL=${DOWNLOAD_URL}" >> $GITHUB_ENV
              break
            elif [ "$STATUS" = "failed" ]; then
              echo "âŒ Backup failed"
              exit 1
            fi
            
            sleep 30
            ELAPSED=$((ELAPSED + 30))
          done
          
          if [ $ELAPSED -ge $MAX_WAIT ]; then
            echo "âš ï¸ Backup timeout"
            exit 1
          fi

      - name: Download backup
        if: env.DOWNLOAD_URL != ''
        run: |
          BACKUP_FILE="backups/supabase_backup_${TIMESTAMP}.sql.gz"
          
          echo "Downloading backup..."
          curl -L "${DOWNLOAD_URL}" -o "${BACKUP_FILE}"
          
          if [ -f "${BACKUP_FILE}" ]; then
            ls -lh "${BACKUP_FILE}"
            echo "âœ… Download successful"
            echo "BACKUP_FILE=${BACKUP_FILE}" >> $GITHUB_ENV
          else
            echo "âŒ Download failed"
            exit 1
          fi

      - name: Upload to Cloudflare R2
        if: env.BACKUP_FILE != ''
        env:
          R2_ACCOUNT_ID: ${{ secrets.R2_ACCOUNT_ID }}
          R2_BUCKET_NAME: ${{ secrets.R2_BUCKET_NAME }}
        run: |
          ENDPOINT_URL="https://${R2_ACCOUNT_ID}.r2.cloudflarestorage.com"
          
          echo "Uploading ${BACKUP_FILE} to R2..."
          
          aws s3 cp \
            "${BACKUP_FILE}" \
            "s3://${R2_BUCKET_NAME}/$(basename ${BACKUP_FILE})" \
            --endpoint-url="${ENDPOINT_URL}"
          
          if [ $? -eq 0 ]; then
            echo "âœ… Upload to R2 successful"
          else
            echo "âŒ Upload to R2 failed"
            exit 1
          fi

      - name: Cleanup old backups from R2 (keep last 30 days)
        env:
          R2_ACCOUNT_ID: ${{ secrets.R2_ACCOUNT_ID }}
          R2_BUCKET_NAME: ${{ secrets.R2_BUCKET_NAME }}
        run: |
          ENDPOINT_URL="https://${R2_ACCOUNT_ID}.r2.cloudflarestorage.com"
          CUTOFF_DATE=$(date -d '30 days ago' +%Y%m%d 2>/dev/null || date -v-30d +%Y%m%d)
          
          echo "Removing backups older than ${CUTOFF_DATE}..."
          
          aws s3 ls "s3://${R2_BUCKET_NAME}/" --endpoint-url="${ENDPOINT_URL}" | \
          while read -r line; do
            BACKUP_NAME=$(echo "$line" | awk '{print $4}')
            
            if [[ "$BACKUP_NAME" =~ ^supabase_backup_([0-9]{8})_ ]]; then
              BACKUP_DATE="${BASH_REMATCH[1]}"
              
              if [ "$BACKUP_DATE" -lt "$CUTOFF_DATE" ]; then
                echo "Deleting old backup: ${BACKUP_NAME}"
                aws s3 rm "s3://${R2_BUCKET_NAME}/${BACKUP_NAME}" --endpoint-url="${ENDPOINT_URL}"
              fi
            fi
          done
          
          echo "âœ… Cleanup completed"

      - name: Backup summary
        if: always()
        run: |
          echo "## ðŸ“Š Backup Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Timestamp:** $(date)" >> $GITHUB_STEP_SUMMARY
          echo "- **Method:** Supabase Management API" >> $GITHUB_STEP_SUMMARY
          echo "- **Backup File:** ${BACKUP_FILE:-N/A}" >> $GITHUB_STEP_SUMMARY
          
          if [ -f "${BACKUP_FILE}" ]; then
            FILE_SIZE=$(ls -lh "${BACKUP_FILE}" | awk '{print $5}')
            echo "- **File Size:** ${FILE_SIZE}" >> $GITHUB_STEP_SUMMARY
            echo "- **Status:** âœ… Success" >> $GITHUB_STEP_SUMMARY
          else
            echo "- **Status:** âŒ Failed" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### âš ï¸ Note" >> $GITHUB_STEP_SUMMARY
            echo "This workflow requires Supabase Management API access." >> $GITHUB_STEP_SUMMARY
            echo "If you don't have access, consider using the pg_dump method with IPv4 connectivity." >> $GITHUB_STEP_SUMMARY
          fi
